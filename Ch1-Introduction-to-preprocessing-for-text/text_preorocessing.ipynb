{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "- Reduce featres\n",
    "- Cleaner, more representative datasets\n",
    "\n",
    "### Tools\n",
    "- Tensorflow\n",
    "- NLTK\n",
    "\n",
    "### Techniques\n",
    "- Tokenization\n",
    "- Stop word removal\n",
    "- Stemming\n",
    "- Rate word removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraxt tokens from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'biggest', 'thing', 'that', 'i', \"'\", 've', 'seen', ',', 'which', 'is', 'absolutely', 'takes', 'me', 'to', 'my', 'core', ',', 'is', 'actually', 'not', 'so', 'much', 'about', 'how', 'humanlike', 'ada', 'is', ',', 'but', 'how', 'robotic', 'we', 'are', '.', 'the', 'algorithms', 'that', 'run', 'our', 'systems', 'are', 'extremely', 'able', 'to', 'be', 'analyzed', ',', 'understood', 'algorithms', 'will', 'know', 'is', 'better', 'than', 'ourselves', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\data\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(\"The biggest thing that I've seen, which is absolutely takes me to my core, is actually not so much about how humanlike ada is, but how robotic we are. The algorithms that run our systems are extremely able to be analyzed, understood algorithms will know is better than ourselves.\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop word removal\n",
    "- Eliminate common words that don't contribute to the meaning\n",
    "- Stop words: a, the, and, or..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\__init__.py:169: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['biggest', 'thing', \"'\", 'seen', ',', 'absolutely', 'takes', 'core', ',', 'actually', 'much', 'humanlike', 'ada', ',', 'robotic', '.', 'algorithms', 'run', 'systems', 'extremely', 'able', 'analyzed', ',', 'understood', 'algorithms', 'know', 'better', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "- Reduce words to their base form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['biggest', 'thing', \"'\", 'seen', ',', 'absolut', 'take', 'core', ',', 'actual', 'much', 'humanlik', 'ada', ',', 'robot', '.', 'algorithm', 'run', 'system', 'extrem', 'abl', 'analyz', ',', 'understood', 'algorithm', 'know', 'better', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infrequent rare words\n",
    "- removing infrequent words that don't add value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', ',', ',', '.', 'algorithm', ',', 'algorithm', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "freq_dist = FreqDist(stemmed_tokens)\n",
    "thresh = 1\n",
    "common_tokens = [token for token in stemmed_tokens if freq_dist[token]> thresh]\n",
    "print(common_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n",
    "Word frequency analysis\n",
    "\n",
    "Congratulations! You've just joined PyBooks. PyBooks is developing a book recommendation system and they want to find patterns and trends in text to improve their recommendations.\n",
    "\n",
    "To begin, you'll want to understand the frequency of words in a given text and remove any rare words.\n",
    "\n",
    "Note that typical real-world datasets will be larger than this example.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Import the tokenization function from torchtext and frequency distribution function from the nltk library.\n",
    "    Initialize the tokenizer for English and tokenize the given text.\n",
    "    Calculate the frequency distribution of the tokens and remove rare words using list comprehension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'of', ',', 'data', 'alex', 'data', '.', ',', 'alex', ',', 'the', 'data', ',', '.', 'the', 'of', '.']\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary functions\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "text = \"In the city of Dataville, a data analyst named Alex explores hidden insights within vast data. With determination, Alex uncovers patterns, cleanses the data, and unlocks innovation. Join this adventure to unleash the power of data-driven decisions.\"\n",
    "\n",
    "# Initialize the tokenizer and tokenize the text\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "threshold = 1\n",
    "# Remove rare words and print common tokens\n",
    "freq_dist = FreqDist(tokens)\n",
    "common_tokens = [token for token in tokens if freq_dist[token] > threshold]\n",
    "print(common_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing text\n",
    "\n",
    "Building a recommendation system, or any model, requires text to be preprocessed first.\n",
    "\n",
    "A block of text from Sherlock Holmes is loaded here. Preprocess this text using the various techniques presented in the video to prepare it for further analysis.\n",
    "\n",
    "The text variable is an excerpt from The Hound of the Baskervilles by Arther Conan Doyle.\n",
    "\n",
    "The following packages and functions have been loaded for you: nltk, torch, get_tokenizer, PorterStemmer, stopwords.\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "\n",
    "    Initialize the tokenizer with \"basic_english\".\n",
    "    Tokenize the text using the tokenizer.\n",
    "    \n",
    "    Create a set of English stopwords and use list comprehension to filter these stop_words out of the text, making sure to ignore capitalization.\n",
    "\n",
    "    Perform stemming on the filtered_tokens using the appropriate nltk function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['citi', 'datavil', ',', 'data', 'analyst', 'name', 'alex', 'explor', 'hidden', 'insight', 'within', 'vast', 'data', '.', 'determin', ',', 'alex', 'uncov', 'pattern', ',', 'cleans', 'data', ',', 'unlock', 'innov', '.', 'join', 'adventur', 'unleash', 'power', 'data-driven', 'decis', '.']\n"
     ]
    }
   ],
   "source": [
    "# Initialize and tokenize the text\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "# Remove any stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# Perform stemming on the filtered tokens\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "print(stemmed_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
