{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "- convert text into machine-readable numbers\n",
    "- enable analysis and modeling\n",
    "\n",
    "### Encoding techniques\n",
    "\n",
    "Use one technique\n",
    "- One-hot encoding: unique numerical representations\n",
    "    - 1 for the presnce\n",
    "    - 0 for the abscence\n",
    "- Bag of words (BoW): captures word freqency, disregarding order\n",
    "    - treat doc. as an unordered collection of words\n",
    "    - ro focus on frequency not order\n",
    "- TF-IDF: balances uniqueness and importance\n",
    "    - term frequency-inverse documnet frecuency\n",
    "    - rare words have a higher score.\n",
    "    - common ones have a lower score.\n",
    "    - emphsizes the important ones\n",
    "- Embeding: converts words into vectors, capturing semantic meaning. maps words to numerical vectors. Example: king and queen, man and woman.\n",
    "    - word inex mapping: king -> 1, queen-> 2\n",
    "    - more compact and computationaly efficient\n",
    "    - follows tokenization typically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': tensor([1., 0., 0.]), 'dog': tensor([0., 1., 0.]), 'rabbit': tensor([0., 0., 1.])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "vocab = ['cat', 'dog','rabbit']\n",
    "vocab_size = len(vocab)\n",
    "one_hot_vectors = torch.eye(vocab_size)\n",
    "one_hot_dict = {word: one_hot_vectors[i] for i, word in enumerate(vocab)}\n",
    "print(one_hot_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW (CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 1 3 1 0 1 0 1 0 0\n",
      "  0 1 1 0 1 0]\n",
      " [0 0 0 1 0 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1\n",
      "  1 1 1 2 1 0]\n",
      " [1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 0 1 0\n",
      "  1 0 1 0 0 1]]\n",
      "['ai' 'algorithms' 'allow' 'and' 'artificial' 'behavior' 'calculations'\n",
      " 'can' 'collective' 'combining' 'computers' 'concerned' 'create' 'data'\n",
      " 'enhance' 'experience' 'google' 'group' 'ideas' 'information' 'insights'\n",
      " 'intelligence' 'is' 'learn' 'learning' 'machine' 'new' 'novel' 'of' 'or'\n",
      " 'pagerank' 'people' 'perform' 'preferences' 'subfield' 'take' 'that'\n",
      " 'the' 'to' 'user' 'which' 'with']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizzer = CountVectorizer()\n",
    "corpus = ['Collective intelligence: which is the combining of behavior, preferences, or ideas of a group of people to create novel insights.',\n",
    "'Googleâ€™s PageRank: which take user data and perform calculations to create new information that can enhance the user experience.'\n",
    ",'Machine Learning: is a subfield of artificial intelligence (AI) concerned with algorithms that allow computers to learn. '\n",
    "]\n",
    "x = vectorizzer.fit_transform(corpus)\n",
    "print(x.toarray())\n",
    "print(vectorizzer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.23283269\n",
      "  0.         0.         0.23283269 0.23283269 0.         0.\n",
      "  0.17707526 0.         0.         0.         0.         0.23283269\n",
      "  0.23283269 0.         0.23283269 0.17707526 0.17707526 0.\n",
      "  0.         0.         0.         0.23283269 0.53122579 0.23283269\n",
      "  0.         0.23283269 0.         0.23283269 0.         0.\n",
      "  0.         0.17707526 0.13751474 0.         0.17707526 0.        ]\n",
      " [0.         0.         0.         0.23148133 0.         0.\n",
      "  0.23148133 0.23148133 0.         0.         0.         0.\n",
      "  0.17604751 0.23148133 0.23148133 0.23148133 0.23148133 0.\n",
      "  0.         0.23148133 0.         0.         0.         0.\n",
      "  0.         0.         0.23148133 0.         0.         0.\n",
      "  0.23148133 0.         0.23148133 0.         0.         0.23148133\n",
      "  0.17604751 0.17604751 0.1367166  0.46296265 0.17604751 0.        ]\n",
      " [0.27054288 0.27054288 0.27054288 0.         0.27054288 0.\n",
      "  0.         0.         0.         0.         0.27054288 0.27054288\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.20575483 0.20575483 0.27054288\n",
      "  0.27054288 0.27054288 0.         0.         0.20575483 0.\n",
      "  0.         0.         0.         0.         0.27054288 0.\n",
      "  0.20575483 0.         0.15978698 0.         0.         0.27054288]]\n",
      "['ai' 'algorithms' 'allow' 'and' 'artificial' 'behavior' 'calculations'\n",
      " 'can' 'collective' 'combining' 'computers' 'concerned' 'create' 'data'\n",
      " 'enhance' 'experience' 'google' 'group' 'ideas' 'information' 'insights'\n",
      " 'intelligence' 'is' 'learn' 'learning' 'machine' 'new' 'novel' 'of' 'or'\n",
      " 'pagerank' 'people' 'perform' 'preferences' 'subfield' 'take' 'that'\n",
      " 'the' 'to' 'user' 'which' 'with']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizzer = TfidfVectorizer()\n",
    "y = vectorizzer.fit_transform(corpus)\n",
    "print(y.toarray())\n",
    "print(vectorizzer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The embedings encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.4661, -0.6180, -1.5599, -0.8083, -1.1261,  0.2165,  0.6045, -0.2697,\n",
      "          1.3442,  0.4469],\n",
      "        [-0.6772, -1.3861, -1.0436, -0.6108, -0.4254,  0.6196,  0.0470, -0.7417,\n",
      "         -0.6104,  0.3865],\n",
      "        [-0.5603, -0.4499,  0.2457, -0.7004, -1.9934,  0.8634,  1.3279,  1.4211,\n",
      "          0.5491, -1.2899],\n",
      "        [-0.2726, -1.2696,  0.9959, -0.2407, -0.2018,  1.3897, -0.0850, -0.6953,\n",
      "          1.7704,  0.2885],\n",
      "        [-0.6623, -0.5446, -0.8054,  0.3834,  0.1954,  1.4201,  0.7841,  0.8725,\n",
      "         -0.1405, -2.2205],\n",
      "        [-0.4203, -0.2566, -0.7075,  0.2712, -2.6097, -2.0456, -1.3913,  0.6208,\n",
      "          0.6186, -1.3626],\n",
      "        [ 0.6556,  0.6496,  0.3189, -0.1471, -1.0145,  0.0521, -2.1341,  0.4171,\n",
      "         -0.0328,  0.1784],\n",
      "        [-0.5188, -1.3143,  1.5656,  0.2504, -0.9893, -0.1764,  0.7010, -0.7542,\n",
      "          0.9931, -0.1539]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "words = [\"the\", \"calico\",\"cat\",\"is\",\"very\",\"beautiful\", \"sat\", \"mat\"]\n",
    "word_to_idx = {word: i for i, word in enumerate(words)}\n",
    "inputs = torch.LongTensor([word_to_idx[w] for w in words])\n",
    "embeddings = nn.Embedding(num_embeddings=len(words), embedding_dim=10)\n",
    "output = embeddings(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n",
    "\n",
    "One-hot encoded book titles\n",
    "\n",
    "PyBooks wants to catalog and analyze the book genres in its library. Apply one-hot encoding to a list of book genres to make them machine-readable.\n",
    "\n",
    "torch has been imported for you.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Define the size of the vocabulary and save to vocab_size.\n",
    "    Create one-hot vectors using the appropriate torch technique and vocab_size.\n",
    "    Create a dictionary mapping genres to their corresponding one-hot vectors using dictionary comprehension; the dictionary keys should be the genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiction: [1. 0. 0. 0. 0.]\n",
      "Non-fiction: [0. 1. 0. 0. 0.]\n",
      "Biography: [0. 0. 1. 0. 0.]\n",
      "Children: [0. 0. 0. 1. 0.]\n",
      "Mystery: [0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "genres = ['Fiction','Non-fiction','Biography', 'Children','Mystery']\n",
    "\n",
    "# Define the size of the vocabulary\n",
    "vocab_size = len(genres)\n",
    "\n",
    "# Create one-hot vectors\n",
    "one_hot_vectors = torch.eye(vocab_size)\n",
    "\n",
    "# Create a dictionary mapping genres to their one-hot vectors\n",
    "one_hot_dict = {genre: one_hot_vectors[i] for i, genre in enumerate(genres)}\n",
    "\n",
    "for genre, vector in one_hot_dict.items():\n",
    "    print(f'{genre}: {vector.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-words for book titles\n",
    "\n",
    "PyBooks now has a list of book titles that need to be encoded for further analysis. The data team believes the Bag of Words (BoW) model could be the best approach.\n",
    "\n",
    "The following packages have been imported for you: torch, torchtext.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Import the CountVectorizer class for implementing bag-of-words.\n",
    "    Initialize an object of the class you imported, then use this object to transform the titles into a matrix representation.\n",
    "    Extract and display the first five feature names and encoded titles with the get_feature_names_out() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1984' 'catcher' 'expectations' 'gatsby' 'great']\n",
      "[0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Import from sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "titles = ['The Great Gatsby','To Kill a Mockingbird','1984','The Catcher in the Rye','The Hobbit', 'Great Expectations']\n",
    "\n",
    "# Initialize Bag-of-words with the list of book titles\n",
    "vectorizer = CountVectorizer()\n",
    "bow_encoded_titles = vectorizer.fit_transform(titles)\n",
    "\n",
    "# Extract and print the first five features\n",
    "print(vectorizer.get_feature_names_out()[:5])\n",
    "print(bow_encoded_titles.toarray()[0, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying TF-IDF to book descriptions\n",
    "\n",
    "PyBooks has collected several book descriptions and wants to identify important words within them using the TF-IDF encoding technique. By doing this, they hope to gain more insights into the unique attributes of each book to help with their book recommendation system.\n",
    "\n",
    "The following packages have been imported for you: torch, torchtext.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Import the class from sklearn.feature_extraction.text that converts a collection of raw documents to a matrix of TF-IDF features.\n",
    "    Instantiate an object of this class, then use this object to encode the descriptions into a TF-IDF matrix of vectors.\n",
    "    Retrieve and display the first five feature names from the vectorizer and encoded vectors from tfidf_encoded_descriptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1984' 'catcher' 'expectations' 'gatsby' 'great']\n",
      "[0.         0.         0.         0.68172171 0.55902156]\n"
     ]
    }
   ],
   "source": [
    "# Importing TF-IDF from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF encoding vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_encoded_descriptions = vectorizer.fit_transform(titles)\n",
    "\n",
    "# Extract and print the first five features\n",
    "print(vectorizer.get_feature_names_out()[:5])\n",
    "print(tfidf_encoded_descriptions.toarray()[0, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding in PyTorch\n",
    "\n",
    "PyBooks found success with a book recommendation system. However, it doesn't account for some of the semantics found in the text. PyTorch's built-in embedding layer can learn and represent the relationship between words directly from data. Your team is curious to explore this capability to improve the book recommendation system. Can you help implement it?\n",
    "\n",
    "torch and torch.nn as nn have been imported for you.\n",
    "Instructions\n",
    "\n",
    "    Map a unique index to each word in words, saving to word_to_idx.\n",
    "    Convert word_to_idx into a PyTorch tensor and save to inputs.\n",
    "    Initialize an embedding layer using the torch module with ten dimensions.\n",
    "    Pass the inputs tensor to the embedding layer and review the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.7160e-01,  1.6860e+00, -1.0942e+00, -1.2422e+00, -6.9444e-02,\n",
      "          2.6087e+00, -8.3396e-01, -1.6440e-01, -1.2661e+00, -6.3702e-01],\n",
      "        [-3.7453e-01, -1.9003e+00,  7.8607e-02, -1.7111e+00,  9.1807e-01,\n",
      "          1.6506e+00,  3.4699e-01, -7.8172e-01,  1.4753e+00, -1.6265e+00],\n",
      "        [ 1.2198e+00,  6.1211e-01,  3.7045e-01, -1.4451e+00,  4.1161e-03,\n",
      "         -6.5725e-01, -2.8760e-01, -3.0660e-02, -2.3058e+00,  7.1787e-01],\n",
      "        [ 1.9828e-02, -1.5543e+00, -1.1705e+00,  7.3115e-01, -5.2529e-01,\n",
      "         -5.7655e-04,  1.8865e+00, -1.9242e+00, -1.0738e-01, -1.1423e+00],\n",
      "        [-4.5352e-01, -9.4602e-02,  7.8129e-01, -2.9492e+00, -1.6859e-01,\n",
      "         -8.5185e-01,  1.4575e-01, -4.8461e-01,  8.1060e-01, -1.2035e-01],\n",
      "        [ 2.4926e-01, -9.2849e-01,  2.0370e+00,  1.6821e+00,  7.7106e-01,\n",
      "          2.1014e+00,  5.5311e-01, -1.0334e-01, -2.2921e+00,  1.3842e-01],\n",
      "        [ 5.8318e-01, -7.6713e-01, -3.8975e-01,  5.4291e-01, -1.4369e+00,\n",
      "          4.0370e-01, -1.8750e-01,  1.8392e+00, -2.3428e-01,  8.8028e-01],\n",
      "        [-3.7962e-01,  9.6889e-01,  7.9165e-01,  3.7385e-01, -7.1866e-01,\n",
      "          5.0669e-03, -1.4117e+00, -2.2980e-01,  5.3200e-01,  9.1047e-02],\n",
      "        [-9.1551e-01, -1.2183e+00, -2.5060e-01, -1.3870e-02, -7.5244e-01,\n",
      "          3.9065e-01,  3.2575e-01,  1.8361e-01,  6.2320e-01, -9.0936e-01],\n",
      "        [-1.0630e+00, -1.0998e+00, -1.2550e+00, -1.3949e+00, -3.3648e-02,\n",
      "          1.0694e+00,  6.5855e-01, -1.2302e-01,  4.0088e-01, -1.6799e+00],\n",
      "        [ 6.2108e-01, -1.1809e+00,  6.9784e-01, -1.8001e+00,  1.2358e+00,\n",
      "          1.9475e-01,  1.1043e+00, -1.9053e+00, -1.2486e+00, -2.0395e-01],\n",
      "        [-1.6760e-01,  7.2616e-01, -2.1298e+00,  4.8360e-01, -2.6763e-01,\n",
      "          3.5041e-01,  3.3863e-01,  1.7261e-02, -1.2195e+00,  9.1421e-01],\n",
      "        [ 1.2198e+00,  6.1211e-01,  3.7045e-01, -1.4451e+00,  4.1161e-03,\n",
      "         -6.5725e-01, -2.8760e-01, -3.0660e-02, -2.3058e+00,  7.1787e-01],\n",
      "        [-3.0232e-01,  9.9591e-01, -2.4785e-01, -1.2076e+00,  8.4237e-01,\n",
      "          4.0562e-01, -2.1029e-01, -8.8647e-01, -3.4826e-01,  3.3242e-01],\n",
      "        [-3.3236e-01, -7.8009e-02, -1.3009e+00, -7.2568e-02,  3.4019e-01,\n",
      "          1.5739e+00, -1.0440e+00, -3.9379e-02, -1.6045e+00, -2.0964e-01]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Map a unique index to each word\n",
    "words = [\"This\", \"book\", \"was\", \"fantastic\", \"I\", \"really\", \"love\", \"science\", \"fiction\", \"but\", \"the\", \"protagonist\", \"was\", \"rude\", \"sometimes\"]\n",
    "word_to_idx = {word: i for i, word in enumerate(words)}\n",
    "\n",
    "# Convert word_to_idx to a tensor\n",
    "inputs = torch.LongTensor([word_to_idx[w] for w in words])\n",
    "\n",
    "# Initialize embedding layer with ten dimensions\n",
    "embedding = nn.Embedding(num_embeddings=len(words), embedding_dim=10)\n",
    "\n",
    "# Pass the tensor to the embedding layer\n",
    "output = embedding(inputs)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
