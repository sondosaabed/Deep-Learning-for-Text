{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://github.com/sondosaabed/SP.TOP-Data-Science-and-Analytics/assets/65151701/c4d8ed09-3b36-4829-b5dc-eb74f17b424c)\n",
    "\n",
    "### Objectives\n",
    "- dataset as container\n",
    "- dataloader: batching, shuffling, multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    def __getitem__(self, index):\n",
    "        return self.text[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentences(sentences):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        tokens = tokenizer(sentence)\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        freq_dist = FreqDist(tokens)\n",
    "        threshold = 2\n",
    "        tokens = [token for token in tokens if freq_dist[token] > threshold]\n",
    "        processed_sentences.append(' '.join(tokens))\n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(sentences):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    encoded_sentences = X.toarray()\n",
    "    return encoded_sentences, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(data):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    sentences = re.findall(r'[A-Z][^.!?]*[.!?]', data)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The whole text processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing_pipeline(text):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    tokens = preprocess_sentences(text)\n",
    "    encoded_sentences, vectorizer = encode_sentences(tokens)\n",
    "    dataset = TextDataset(encoded_sentences)\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    return dataloader, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m text_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is the first text data.  And here is another one. And here is another one.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m sentences \u001b[38;5;241m=\u001b[39m extract_sentences(text_data)\n\u001b[1;32m----> 6\u001b[0m dataloaders, vectorizer \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mtext_processing_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataloader))[\u001b[38;5;241m0\u001b[39m, :\u001b[38;5;241m10\u001b[39m])\n",
      "Cell \u001b[1;32mIn[27], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m text_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is the first text data.  And here is another one. And here is another one.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m sentences \u001b[38;5;241m=\u001b[39m extract_sentences(text_data)\n\u001b[1;32m----> 6\u001b[0m dataloaders, vectorizer \u001b[38;5;241m=\u001b[39m [\u001b[43mtext_processing_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m sentences]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataloader))[\u001b[38;5;241m0\u001b[39m, :\u001b[38;5;241m10\u001b[39m])\n",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m, in \u001b[0;36mtext_processing_pipeline\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext_processing_pipeline\u001b[39m(text):\n\u001b[0;32m      2\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m preprocess_sentences(text)\n\u001b[1;32m----> 3\u001b[0m     encoded_sentences, vectorizer \u001b[38;5;241m=\u001b[39m \u001b[43mencode_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m TextDataset(encoded_sentences)\n\u001b[0;32m      5\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m, in \u001b[0;36mencode_sentences\u001b[1;34m(sentences)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_sentences\u001b[39m(sentences):\n\u001b[0;32m      2\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[1;32m----> 3\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     encoded_sentences \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_sentences, vectorizer\n",
      "File \u001b[1;32mc:\\Users\\SS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1377\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1369\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1374\u001b[0m             )\n\u001b[0;32m   1375\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1377\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1380\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\SS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1283\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1281\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1284\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1285\u001b[0m         )\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "dataset = TextDataset(\"This is the first text data.  And here is another one. And here is another one.\")\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "text_data = \"This is the first text data.  And here is another one. And here is another one.\"\n",
    "sentences = extract_sentences(text_data)\n",
    "dataloaders, vectorizer = [text_processing_pipeline(text) for text in sentences]\n",
    "print(next(iter(dataloader))[0, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n",
    "\n",
    "Shakespearean language preprocessing pipeline\n",
    "\n",
    "Over at PyBooks, the team wants to transform a vast library of Shakespearean text data for further analysis. The most efficient way to do this is with a text processing pipeline, starting with the preprocessing steps.\n",
    "\n",
    "The following have been loaded for you: torch, nltk, stopwords, PorterStemmer, get_tokenizer.\n",
    "\n",
    "The Shakespearean text data is saved as shakespeare and the sentences have already been extracted.\n",
    "\n",
    "    Create a list of unique English stopwords, saving to them to stop_words.\n",
    "\n",
    "    Initialize the basic_english tokenizer from torch, and PorterStemmer from nltk.\n",
    "\n",
    "    Complete the preprocess_sentences() function to enable tokenization, stop word removal, and stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c', '', '', 'p', 'l']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize the tokenizer and stemmer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "# Complete the function to preprocess sentences\n",
    "def preprocess_sentences(sentences):\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        tokens = tokenizer(sentence)\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        processed_sentences.append(' '.join(tokens))\n",
    "    return processed_sentences\n",
    "\n",
    "processed_shakespeare = preprocess_sentences(\"Complete the function to preprocess sentences. Initialize the tokenizer and stemmer \")\n",
    "print(processed_shakespeare[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hakespearean language encoder\n",
    "\n",
    "With the preprocessed Shakespearean text at your fingertips, you now need to encode it into a numerical representation. You will need to define the encoding steps before putting the pipeline together. To better handle large amounts of data and efficiently perform the encoding, you will use PyTorch's Dataset and DataLoader for batching and shuffling the data.\n",
    "\n",
    "The following has been loaded for you: torch, nltk, stopwords, PorterStemmer, get_tokenizer, CountVectorizer, Dataset, and DataLoader.\n",
    "\n",
    "The processed_shakespeare from the Shakespearean text is also available to you.\n",
    "\n",
    "    Define a ShakespeareDataset dataset class and complete the __init__ and __getitem__ methods.\n",
    "    Complete the encode_sentences() function to take in a list of sentences and encode them using the bag-of-words technique from sklearn.\n",
    "    \n",
    "    Complete and call the text_processing_pipeline() function by using preprocess_sentences(), encode_sentences(), ShakespeareDataset class, and DataLoader.\n",
    "    Print the first ten feature names with the get_feature_names_out() method and components of the first item of dataloader.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataloader, vectorizer\n\u001b[1;32m---> 24\u001b[0m dataloader, vectorizer \u001b[38;5;241m=\u001b[39m \u001b[43mtext_processing_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_shakespeare\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Print the vectorizer's feature names and the first 10 components of the first item\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()[:\u001b[38;5;241m10\u001b[39m]) \n",
      "Cell \u001b[1;32mIn[36], line 19\u001b[0m, in \u001b[0;36mtext_processing_pipeline\u001b[1;34m(sentences)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext_processing_pipeline\u001b[39m(sentences):\n\u001b[0;32m     18\u001b[0m     processed_sentences \u001b[38;5;241m=\u001b[39m preprocess_sentences(sentences)\n\u001b[1;32m---> 19\u001b[0m     encoded_sentences, vectorizer \u001b[38;5;241m=\u001b[39m \u001b[43mencode_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_sentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m ShakespeareDataset(encoded_sentences)\n\u001b[0;32m     21\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[36], line 13\u001b[0m, in \u001b[0;36mencode_sentences\u001b[1;34m(sentences)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_sentences\u001b[39m(sentences):\n\u001b[0;32m     12\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[1;32m---> 13\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\u001b[38;5;241m.\u001b[39mtoarray(), vectorizer\n",
      "File \u001b[1;32mc:\\Users\\SS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1377\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1369\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1374\u001b[0m             )\n\u001b[0;32m   1375\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1377\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1380\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\SS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1283\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1281\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1284\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1285\u001b[0m         )\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# Define your Dataset class\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "# Complete the encoding function\n",
    "def encode_sentences(sentences):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    return X.toarray(), vectorizer\n",
    "\n",
    "# Complete the text processing pipeline\n",
    "def text_processing_pipeline(sentences):\n",
    "    processed_sentences = preprocess_sentences(sentences)\n",
    "    encoded_sentences, vectorizer = encode_sentences(processed_sentences)\n",
    "    dataset = ShakespeareDataset(encoded_sentences)\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    return dataloader, vectorizer\n",
    "\n",
    "dataloader, vectorizer = text_processing_pipeline(processed_shakespeare)\n",
    "\n",
    "# Print the vectorizer's feature names and the first 10 components of the first item\n",
    "print(vectorizer.get_feature_names_out()[:10]) \n",
    "print(next(iter(dataloader))[0, :10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
