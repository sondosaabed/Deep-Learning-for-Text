{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative adversial networks for text generation\n",
    "- often used for image generation\n",
    "- are becoming more comon for text generation\n",
    "- creating synthetic data for preserving statistical similarities\n",
    "- unike rnns gans replicate complex data patterns\n",
    "- ensuring feature correlation\n",
    "- authentically emulating real-world patterns\n",
    "\n",
    "### Structure of a GAN\n",
    "- Has two components\n",
    "    - Generator: creates fake samples by adding noise.\n",
    "    - discrimentator: diffrentiates between real and generated text data.\n",
    "- noise in here means random changes to real data, such as adding special charchters to a word.\n",
    "- These componets collaborate with teh gerntaor imporiving it's fakes and the discrimnator enhancing it's ability to detect them until the generated text becomes indistingishable from real text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gnerator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(sequence_length, sequence_length),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrimnator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(sequence_length, 1),\n",
    "            nn.Sigmoid() ## is the synthetic data real\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "generator = Generator()\n",
    "discrinitor = Discriminator()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "opt_gen = torch.optim.Adam(generator.parameters(), lr=0.001)\n",
    "opt_disc = torch.optim.Adam(discrinitor.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traininng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for real_data in data:\n",
    "        real_data - real_data.unsqueeze(0)\n",
    "        noise = torch.rand((1, sequence_length))\n",
    "\n",
    "        disc_real = discrinitor(real_data)\n",
    "        fake_data = generator(noise)\n",
    "\n",
    "        disc_fake = discrinitor(fake_data.detach()) #detach is used to prevent gradiant tracking\n",
    "        loss_disc = criterion(disc_real, torch.ones_like(disc_real)) + criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "\n",
    "        opt_disc.zero_grad()\n",
    "        loss_disc.backward()\n",
    "        opt_disc.step()\n",
    "\n",
    "        ## train the generator\n",
    "        disc_fake = discrinitor(fake_data)\n",
    "        loss_gen = criterion(disc_fake, torch.ones_like(disc_fake))\n",
    "        opt_gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\\t\n",
    "        Generator loss: {loss_gen.item()}\\t\n",
    "        Discriminator loss: {loss_disc.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Real data: \")\n",
    "print(data[:5])\n",
    "\n",
    "print(\"Generated data: \")\n",
    "for _ in range(5):\n",
    "    noise = torch.rand((1, sequence_length))\n",
    "    generated_data = generator(noise)\n",
    "    print(torch.round(generated_data).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the output is also tensors.\n",
    "- in practice we also print the correltion matrix to see if the correlation is maintaned as an evaluation metric \n",
    "\n",
    "![image](https://github.com/sondosaabed/Deep-Learning-for-Text/assets/65151701/414bc504-b50d-407a-b73d-70f6b396cb63)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a generator and discriminator\n",
    "\n",
    "At PyBooks, you're tasked with working on an automatic text generator to help writers overcome writer's block. By using GANs, or Generative Adversarial Networks, you believe you can create a system where one network, the generator, creates new text while the other network, the discriminator, evaluates its authenticity. To do this, you need to initialize both a generator and discriminator network. These networks will then be trained against each other to create new, believable text.\n",
    "\n",
    "    Define the Generator class with a linear layer for sequential data and a sigmoid activation function.\n",
    "    Pass the input through the defined model in the forward() method of the Generator class.\n",
    "    Define a Discriminator class with the same layers and activation function, taking care when defining the dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(seq_length, seq_length), nn.Sigmoid())\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Define the discriminator networks\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(seq_length, 1), nn.Sigmoid())\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a GAN model\n",
    "\n",
    "Your team at PyBooks has made good progress in building the text generator using a Generative Adversarial Network (GAN). You have successfully defined the generator and discriminator networks. Now, it's time to train them. The final step is to generate some fake data and compare it with the real data to see how well your GAN has learned. We have used tensors as an input and the output would try to resemble the input tensors. The team at PyBooks can then use this synthetic data for text analysis as the features will have same relationship as text data.\n",
    "\n",
    "The generator and discriminator have been initialized and saved to generator and discriminator, respectively.\n",
    "\n",
    "The following variables have been initialized in the exercise:\n",
    "\n",
    "    seq_length = 5: Length of each synthetic data sequence\n",
    "    num_sequences = 100: Total number of sequences generated\n",
    "    num_epochs = 50: Number of complete passes through the dataset\n",
    "    print_every = 10: Output display frequency, showing results every 10 epochs\n",
    "\n",
    "\n",
    "    Define the loss function for binary classification and the Adam optimizer.\n",
    "    Train the discriminator by unsqueezing real_data and preventing gradient recalculations.\n",
    "    Train the generator by calculating the loss and zeroing the gradients.\n",
    "    Detach the tensor to prevent further computation and print data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_gen = torch.optim.Adam(generator.parameters(), lr=0.001)\n",
    "optimizer_disc = torch.optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for real_data in data:\n",
    "        # Unsqueezing real_data and prevent gradient recalculations\n",
    "        real_data = real_data.unsqueeze(0)\n",
    "        noise = torch.rand((1, seq_length))\n",
    "        fake_data = generator(noise)\n",
    "        disc_real = discriminator(real_data)\n",
    "        disc_fake = discriminator(fake_data.detach())\n",
    "        loss_disc = criterion(disc_real, torch.ones_like(disc_real)) + criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        optimizer_disc.zero_grad()\n",
    "        loss_disc.backward()\n",
    "        optimizer_disc.step()\n",
    "\n",
    "        # Train the generator\n",
    "        disc_fake = discriminator(fake_data)\n",
    "        loss_gen = criterion(disc_fake, torch.ones_like(disc_fake))\n",
    "        optimizer_gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        optimizer_gen.step()\n",
    "\n",
    "    if (epoch+1) % print_every == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\\t Generator loss: {loss_gen.item()}\\t Discriminator loss: {loss_disc.item()}\")\n",
    "\n",
    "print(\"\\nReal data: \")\n",
    "print(data[:5])\n",
    "\n",
    "print(\"\\nGenerated data: \")\n",
    "for _ in range(5):\n",
    "    noise = torch.rand((1, seq_length))\n",
    "    generated_data = generator(noise)\n",
    "    # Detach the tensor and print data\n",
    "    print(torch.round(generated_data).detach())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
