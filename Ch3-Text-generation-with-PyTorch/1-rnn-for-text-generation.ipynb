{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications\n",
    "- chatbots, langauge translation, technical writing\n",
    "\n",
    "### Text genration with RNN\n",
    "- RNN, LSTM, GRU: remebering past information for better sequential data processing\n",
    "- Example: The cat is on the m-> mat\n",
    "\n",
    "We treat text genration as a regression problem to predict the next token in a sequence as the next token can have infinite tensor output classes rather than a set number of output classes needed for classification.\n",
    "\n",
    "### Preparing input and target data\n",
    "- creating indixes\n",
    "- tensor conversion\n",
    "- one-hot encoding\n",
    "- targets prepartion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "data = \"Hello how ary tp\"\n",
    "\n",
    "chars = list(set(data))\n",
    "char_to_ix = {char: i for i, char in enumerate(chars)}\n",
    "ix_to_char = {i: char for i, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    ## forward propagation\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "input_size = len(chars)  \n",
    "hidden_size = 32\n",
    "output_size = len(chars)  \n",
    "\n",
    "## model creation\n",
    "model = RNNModel(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preparing input and target data\n",
    "inputs = [char_to_ix[ch] for ch in data[:-1]]\n",
    "targets = [char_to_ix[ch] for ch in data[1:]]\n",
    "\n",
    "#tensor conversion \n",
    "inputs = torch.tensor(inputs, dtype=torch.long).view(-1, 1)\n",
    "\n",
    "## one hot encoding\n",
    "inputs = nn.functional.one_hot( inputs, num_classes=len(chars)).float()\n",
    "\n",
    "## targets\n",
    "targets = torch.tensor(targets, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 1.958619236946106\n",
      "Epoch 20/100, Loss: 1.3095365762710571\n",
      "Epoch 30/100, Loss: 0.6994163393974304\n",
      "Epoch 40/100, Loss: 0.47440794110298157\n",
      "Epoch 50/100, Loss: 0.4281369745731354\n",
      "Epoch 60/100, Loss: 0.4168391823768616\n",
      "Epoch 70/100, Loss: 0.412965327501297\n",
      "Epoch 80/100, Loss: 0.4111606180667877\n",
      "Epoch 90/100, Loss: 0.4101131856441498\n",
      "Epoch 100/100, Loss: 0.4093896746635437\n"
     ]
    }
   ],
   "source": [
    "### training the RNN model\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 ==0:\n",
    "        print(f'Epoch {epoch+1}/100, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Input: 'h', Predicted Output: 'o'\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_char = 'h'\n",
    "test_input_ix = char_to_ix[test_char]\n",
    "test_input = nn.functional.one_hot(torch.tensor(test_input_ix).view(-1, 1), num_classes=len(chars)).float()\n",
    "predicted_output = model(test_input)\n",
    "predicted_char_ix = torch.argmax(predicted_output, 1).item()\n",
    "predicted_char = ix_to_char[predicted_char_ix]\n",
    "print(f\"Test Input: '{test_char}', Predicted Output: '{predicted_char}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a RNN model for text generation\n",
    "\n",
    "At PyBooks, you've been tasked to develop an algorithm that can perform text generation. The project involves auto-completion of book names. To kickstart this project, you decide to experiment with a Recurrent Neural Network (RNN). This way, you can understand the nuances of RNNs before moving to more complex models.\n",
    "\n",
    "The following has been imported for you: torch, torch.nn as nn.\n",
    "\n",
    "The data variable has been initialized with an excerpt from Alice's Adventures in Wonderland by Lewis Carroll.\n",
    "\n",
    "    Include an RNN layer and linear layer in RNNmodel class\n",
    "    Instantiate the RNN model with input size as length of chars, hidden size of 16, and output size as length of chars.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Include an RNN layer and linear layer in RNNmodel class\n",
    "class RNNmodel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNmodel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "      h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "      out, _ = self.rnn(x, h0)  \n",
    "      out = self.fc(out[:, -1, :])  \n",
    "      return out\n",
    "\n",
    "# Instantiate the RNN model\n",
    "model = RNNmodel(len(chars), 16, len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text generation using RNN - Training and Generation\n",
    "\n",
    "The team at PyBooks now wants you to train and test the RNN model, which is designed to predict the next character in the sequence based on the provided input for auto-completion of book names. This project will help the team further develop models for text completion.\n",
    "\n",
    "The model instance for the RNNmodel class is preloaded for you. The data variable has been preprocessed and encoded as a sequence.\n",
    "\n",
    "The inputs and targets variable are preloaded for you.\n",
    "\n",
    "    Instantiate the loss function which will be used to compute the error of our model.\n",
    "    Instantiate the optimizer from PyTorch's optimization module.\n",
    "    Run the model training process by setting the model to the train mode and zeroing the gradients before performing an optimization step.\n",
    "    After the training process, switch the model to evaluation mode to test it on a sample input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Instantiate the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/100, Loss: {loss.item()}')\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "test_input = char_to_ix['r']\n",
    "test_input = nn.functional.one_hot(torch.tensor(test_input).view(-1, 1), num_classes=len(chars)).float()\n",
    "predicted_output = model(test_input)\n",
    "predicted_char_ix = torch.argmax(predicted_output, 1).item()\n",
    "print(f\"Test Input: 'r', Predicted Output: '{ix_to_char[predicted_char_ix]}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
