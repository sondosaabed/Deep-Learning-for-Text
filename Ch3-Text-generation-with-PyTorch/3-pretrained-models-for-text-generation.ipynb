{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained models for text generation\n",
    "- offer advantages having being trained on extensive datasets.\n",
    "- high performance accross text generation tasks.\n",
    "    - sentiments analysis\n",
    "    - text completion\n",
    "    - lanaguge translation\n",
    "- Limitiations\n",
    "    - high computaional cost for trainng\n",
    "    - large storage requiremnts\n",
    "    - limited customization options\n",
    "    \n",
    "- Pytoch higging face transformers\n",
    "    - GPT2LMHead model: tasks for text generation\n",
    "    - GPT2Tokenizer: converts text into tokens, handles subword tokenization: 'larger' might becom ['large', 'r']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chosing the right pre-trained model\n",
    "- GPT-2: text generation\n",
    "- DistilGPT-2: (smaller version of GPT-2): text generation\n",
    "- BERT: text classification, question-answering\n",
    "- T5 (t5-small is teh smaller version of T5): lanaguge translation, summerization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2: text generation implemention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "seed_text = \"Once upon a time\"# serve as a story starter\n",
    "input_ids =tokenizer.encode(seed_text, return_tensors='pt') ## pt means in pytorch format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# temrature: controls the randomnes of the output\n",
    "# no_repaet_ngram_size: prevernts consicutive letters\n",
    "# eos: end of sentence token (pad the output if it's shorter than the max length)\n",
    "\n",
    "output = model.generate(input_ids, max_length=40, temperature=0.7, no_repeat_ngram_size=2, pad_token_id = tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the world was a place of great beauty and great danger. The world of the gods was the place where the great gods were born, and where they were to live.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5: Lanaguge translation implemntaion\n",
    "- text-to-text transfer transformer\n",
    "- pretrained model for lanaguge translation tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38804ea5ab574d9aa80aafc29774a6e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SS\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b55e2360b642ffa852048cbfdab812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd7292490f994917864c9996c1a32d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c80f1debbd0435cb05df27d683389f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24f398f949c4185a258ee4fa1ca5b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9dba3dfd5141fe925c60dc1a607d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "input_prompt = \"translate English to French: 'Hello, how are you?'\"\n",
    "input_ids = tokenizer.encode(input_prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(input_ids, max_length= 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \"Jo, comment Ãªtes-vous?\"\n"
     ]
    }
   ],
   "source": [
    "genertaed_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\", genertaed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text completion with pre-trained GPT-2 models\n",
    "\n",
    "Back at PyBooks, your current project involves creating captivating narratives based on existing stories to engage customers and enhance their experience. To achieve this, you need a powerful text generation tool that can seamlessly generate compelling text continuations. You'll be using a pre-trained model to get the job done.\n",
    "\n",
    "The following has been loaded for you: torch and GPT2Tokenizer,GPT2LMHeadModel from transformers.\n",
    "\n",
    "\n",
    "    Initialize the pre-trained GPT-2 tokenizer for gpt2.\n",
    "    Initialize the pre-trained GPT-2 LMHead model for gpt2.\n",
    "    Encode the seed text to get input tensors using seed_text.\n",
    "    Generate the text by defining the parameters for the input considering a max_length of 100, temperature of 0.7, and an no_repeat_ngram_size of 2 in the GPT-2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Initialize the pre-trained model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "seed_text = \"Once upon a time\"\n",
    "\n",
    "# Encode the seed text to get input tensors\n",
    "input_ids = tokenizer.encode(seed_text, return_tensors='pt')\n",
    "\n",
    "# Generate text from the model\n",
    "output = model.generate(input_ids, max_length=100, temperature=0.7, no_repeat_ngram_size=2, pad_token_id=tokenizer.eos_token_id) \n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language translation with pretrained PyTorch model\n",
    "\n",
    "Your team at PyBooks is working on an AI project that involves translation from one language to another. They want to leverage pre-trained models for this task, which can save a lot of training time and resources. The task for this exercise is to set up a translation model from HuggingFace's Transformers library, specifically the T5 (Text-To-Text Transfer Transformer) model, and use it to translate an English phrase to French.\n",
    "\n",
    "T5Tokenizer, T5ForConditionalGeneration have been loaded for you.\n",
    "\n",
    "    Initialize the tokenizer and model from the pretrained \"t5-small\" model.\n",
    "    Encode the input prompt using the tokenizer, making sure to return PyTorch tensors.\n",
    "    Translate the input prompt using model and generate the translated output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "input_prompt = \"translate English to French: 'Hello, how are you?'\"\n",
    "\n",
    "# Encode the input prompt using the tokenizer\n",
    "input_ids = tokenizer.encode(input_prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the translated ouput\n",
    "output = model.generate(input_ids, max_length=50)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\",generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
