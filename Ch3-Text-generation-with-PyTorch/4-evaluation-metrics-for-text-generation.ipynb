{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics for text generation\n",
    "- create human-like text\n",
    "- standard accuravu metrics such as accuracu, f-3 fall short in these tasks\n",
    "- we need metrics like BlEU and ROUGE to evaluate the quality of the generated text\n",
    "\n",
    "### BLEU Bilingual Evaluation Understudy\n",
    "- compares the generated text and the refrence text\n",
    "- checks for the occurence of n-grams: \n",
    "    - 1-grams (uni-gram): [the, cat, is..]\n",
    "    - 2-grams (bi-gram): [tha cat, cat is...]\n",
    "- the more the genrated match the refrnce n-gram the more the higher the BELU scoe\n",
    "    - 1 perfect match, 0 no match\n",
    "\n",
    "### ROUGE Recall-Oriented Understudy for gisting evaluation\n",
    "- cmpares refrence text against generated texts in two ways:\n",
    "    - ROUGE-N: considers overlapping n-grams in both texts\n",
    "    - ROUGE-L: checks the longest subsequnce comomon (LCS) between them\n",
    "- ROUGE has three metrics:\n",
    "    - F-Measure: Harmonic mean of percision and recall\n",
    "    - Precision: Matches of n-grams in generated text within the refrence text\n",
    "    - Recall: Matches of n-grams in refrence text within the generated text.\n",
    "\n",
    "### considerations\n",
    "- Evaluate word presence, not semantic understanding\n",
    "- Sensetive to the length of the generated text\n",
    "- Quality of refrence text affects the scors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PyBooks team has used a pre-trained GPT-2 model that you experimented to generate a text based on a given prompt. Now, they want to evaluate the quality of this generated text. To achieve this, they have tasked you to evaluate generated text using a reference text.\n",
    "\n",
    "BLEUScore, ROUGEScore have been loaded for you.\n",
    "\n",
    "    Begin by initializing the two metrics (BLEU and ROUGE) provided from torchmetrics.text.\n",
    "    Use these initialized metrics to calculate the scores between the generated text and the reference text.\n",
    "    Display the calculated BLEU and ROUGE scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_text = \"Once upon a time, there was a little girl who lived in a village near the forest.\"\n",
    "generated_text = \"Once upon a time, the world was a place of great beauty and great danger. The world of the gods was the place where the great gods were born, and where they were to live.\"\n",
    "\n",
    "# Initialize BLEU and ROUGE scorers\n",
    "bleu = BLEUScore()\n",
    "rouge = ROUGEScore()\n",
    "\n",
    "# Calculate the BLEU and ROUGE scores\n",
    "bleu_score = bleu([generated_text], [[reference_text]])\n",
    "rouge_score = rouge([generated_text], [[reference_text]])\n",
    "\n",
    "# Print the BLEU and ROUGE scores\n",
    "print(\"BLEU Score:\", bleu_score.item())\n",
    "print(\"ROUGE Score:\", rouge_score)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
