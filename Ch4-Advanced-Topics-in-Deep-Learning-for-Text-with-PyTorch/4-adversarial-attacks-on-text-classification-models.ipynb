{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversial attacks on text classification models\n",
    "- making crafted tweaks to input data\n",
    "- not random but calculated malicisios changes\n",
    "- can drastically affect AI's decision-making\n",
    "\n",
    "- might make benign content appear harmfull and vice versa\n",
    "- AI unintentional amplification of negative sterotypes from biased data\n",
    "- misleading information\n",
    "\n",
    "### Fast Gradient Sign Method (FGSM)\n",
    "- used precise changes that may go undeticted by exploiting the model's learning information.\n",
    "- makes tiniest possible change to deceive the model\n",
    "- a spam filter that is usually accurate but got decieved by an email \n",
    "    - they made a tiny change in teh word love \n",
    "\n",
    "![image](https://github.com/community/community/assets/65151701/5d627629-4de5-47c0-b00d-f7e7741025e3)\n",
    "\n",
    "### Projected Gradient Descent (PGD)\n",
    "- a seasoned bargular that breach the lock step by step\n",
    "- move advanced than FGSM: it's iterative\n",
    "- tries to find the most effective disturbance\n",
    "\n",
    "![image](https://github.com/community/community/assets/65151701/28ade2ad-3c24-44e9-8e18-a2f19b10ef08)\n",
    "\n",
    "### The Carlini & Wanger (C&W) attack\n",
    "- like a master mind who leaves no trace\n",
    "- focuses on optimizing the loss function\n",
    "- not just about deciving but about being undetectable\n",
    "- tweaks finiacial transcript subtly so cause erroreuos investments\n",
    "- changes sentemant\n",
    "\n",
    "![image](https://github.com/community/community/assets/65151701/bebfb927-5f80-4a73-86a4-d5741bf3df38)\n",
    "\n",
    "### Building defenses: stratgies\n",
    "- defending against text-based manipulations requires strong strategies\n",
    "- when spotting fake news model ensembling becomes invaluable\n",
    "- relying on the consensus of multiple models allows us to filter out deceptive content with heightened accuracy.\n",
    "    - Model ensemblig: uses multiple models\n",
    "    - robust data augmentaion: exposes them to diffrent paraphrasingof questions which aids in consistent reposne delivery.\n",
    "    - Adversarial training: anticipate deception in reviews, ensure accurate sentiment interpretaion.\n",
    "\n",
    "### Building defenses: tools and techniques \n",
    "- the right tools can make all the diffrence\n",
    "- pytorch adversarial robustness toolbox \n",
    "    - stregthen text models\n",
    "- gradient masking: \n",
    "    - add variety to training data to hide explotable patterns.\n",
    "- regularization techniques: ensure model balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pratice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
