{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversial attacks on text classification models\n",
    "- making crafted tweaks to input data\n",
    "- not random but calculated malicisios changes\n",
    "- can drastically affect AI's decision-making\n",
    "\n",
    "- might make benign content appear harmfull and vice versa\n",
    "- AI unintentional amplification of negative sterotypes from biased data\n",
    "- misleading information\n",
    "\n",
    "### Fast Gradient Sign Method (FGSM)\n",
    "- used precise changes that may go undeticted by exploiting the model's learning information.\n",
    "- makes tiniest possible change to deceive the model\n",
    "- a spam filter that is usually accurate but got decieved by an email \n",
    "    - they made a tiny change in teh word love \n",
    "\n",
    "![image](https://github.com/community/community/assets/65151701/5d627629-4de5-47c0-b00d-f7e7741025e3)\n",
    "\n",
    "### Projected Gradient Descent (PGD)\n",
    "- a seasoned bargular that breach the lock step by step\n",
    "- move advanced than FGSM: it's iterative\n",
    "- tries to find the most effective disturbance\n",
    "\n",
    "![image](https://github.com/community/community/assets/65151701/28ade2ad-3c24-44e9-8e18-a2f19b10ef08)\n",
    "\n",
    "### The Carlini & Wanger (C&W) attack\n",
    "- like a master mind who leaves no trace\n",
    "- focuses on optimizing the loss function\n",
    "- not just about deciving but about being undetectable\n",
    "- tweaks finiacial transcript subtly so cause erroreuos investments\n",
    "- changes sentemant\n",
    "\n",
    "![image](https://github.com/community/community/assets/65151701/bebfb927-5f80-4a73-86a4-d5741bf3df38)\n",
    "\n",
    "### Building defenses: stratgies\n",
    "- defending against text-based manipulations requires strong strategies\n",
    "- when spotting fake news model ensembling becomes invaluable\n",
    "- relying on the consensus of multiple models allows us to filter out deceptive content with heightened accuracy.\n",
    "    - Model ensemblig: uses multiple models\n",
    "    - robust data augmentaion: exposes them to diffrent paraphrasingof questions which aids in consistent reposne delivery.\n",
    "    - Adversarial training: anticipate deception in reviews, ensure accurate sentiment interpretaion.\n",
    "\n",
    "### Building defenses: tools and techniques \n",
    "- the right tools can make all the diffrence\n",
    "- pytorch adversarial robustness toolbox \n",
    "    - stregthen text models\n",
    "- gradient masking: \n",
    "    - add variety to training data to hide explotable patterns.\n",
    "- regularization techniques: ensure model balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pratice\n",
    "Adversarial attack classification\n",
    "\n",
    "Imagine you're a Data Scientist on a mission to safeguard machine learning models from malicious attacks. In order to do so, you need to be aware of the different attacks that you and your model could encounter. Being aware of these vulnerabilities will allow you to protect your models against these adversarial threats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Safeguarding AI at PyBooks\n",
    "\n",
    "You are responsible for implementing a new chatbot at PyBooks to assist users on the platform. However, you've been alerted about potential adversarial attacks against text-based AI systems. To ensure the chatbot remains robust against such threats, you decide to research various strategies and techniques.\n",
    "\n",
    "Which of the following approaches would be LEAST effective in safeguarding the chatbot against adversarial attacks?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
