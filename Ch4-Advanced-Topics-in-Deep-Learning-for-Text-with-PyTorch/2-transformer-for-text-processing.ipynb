{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformers for text processing\n",
    "- speed\n",
    "- understand the relationship between words regardless of the distance\n",
    "- human like texts\n",
    "\n",
    "### transformers components\n",
    "- Encoder: process input data\n",
    "- Decoder: reconstructs the output\n",
    "- feed forward networks: refine understanding\n",
    "- positional encodings: ensure order matters\n",
    "- multi head attention: captures multiple inputs or sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n",
    "\n",
    "Creating a transformer model\n",
    "\n",
    "At PyBooks, the recommendation engine you're working on needs more refined capabilities to understand the sentiments of user reviews. You believe that using transformers, a state-of-the-art architecture, can help achieve this. You decide to build a transformer model that can encode the sentiments in the reviews to kickstart the project.\n",
    "\n",
    "The following packages have been imported for you: torch, nn, optim.\n",
    "\n",
    "The input data contains sentences such as : \"I love this product\", \"This is terrible\", \"Could be better\" â€¦ and their respective binary sentiment labels such as : 1, 0, 0, ...\n",
    "\n",
    "The input data is split and converted to embeddings in the following variables: train_sentences, train_labels ,test_sentences,test_labels,token_embeddings\n",
    "\n",
    "    Initialize the transformer encoder.\n",
    "    Define the fully connected layer based on the number of sentiment classes.\n",
    "    In the forward method, pass the input through the transformer encoder followed by the linear layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_size, heads, num_layers, dropout):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        # Initialize the encoder \n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embed_size, nhead=heads),\n",
    "            num_layers=num_layers)\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(embed_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the transformer encoder \n",
    "        x = self.encoder(x)\n",
    "        x = x.mean(dim=1) \n",
    "        return self.fc(x)\n",
    "\n",
    "model = TransformerEncoder(embed_size=512, heads=8, num_layers=3, dropout=0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing the Transformer model\n",
    "\n",
    "With the TransformerEncoder model in place, the next step at PyBooks is to train the model on sample reviews and evaluate its performance. Training on these sample reviews will help PyBooks understand the sentiment trends in their vast repository. By achieving a well-performing model, PyBooks can then automate sentiment analysis, ensuring readers get insightful recommendations and feedback.\n",
    "\n",
    "The following packages have been imported for you: torch, nn, optim.\n",
    "\n",
    "The model instance of the TransformerEncoder class, token_embeddings, and the train_sentences, train_labels ,test_sentences,test_labels are preloaded for you.\n",
    "\n",
    "    In the training loop, split the sentences into tokens and stack the embeddings.\n",
    "    Zero the gradients and perform a backward pass.\n",
    "    In the predict function, deactivate the gradient computations then get the sentiment prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "for epoch in range(5):  \n",
    "    for sentence, label in zip(train_sentences, train_labels):\n",
    "        # Split the sentences into tokens and stack the embeddings\n",
    "        tokens = sentence.split()\n",
    "        data = torch.stack([token_embeddings[token] for token in tokens], dim=1)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, torch.tensor([label]))\n",
    "        # Zero the gradients and perform a backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "def predict(sentence):\n",
    "    model.eval()\n",
    "    # Deactivate the gradient computations and get the sentiment prediction.\n",
    "    with torch.no_grad():\n",
    "        tokens = sentence.split()\n",
    "        data = torch.stack([token_embeddings.get(token, torch.rand((1, 512))) for token in tokens], dim=1)\n",
    "        output = model(data)\n",
    "        predicted = torch.argmax(output, dim=1)\n",
    "        return \"Positive\" if predicted.item() == 1 else \"Negative\"\n",
    "\n",
    "sample_sentence = \"This product can be better\"\n",
    "print(f\"'{sample_sentence}' is {predict(sample_sentence)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1, Loss: 0.3518843650817871\n",
    "Epoch 1, Loss: 0.3547934889793396\n",
    "Epoch 2, Loss: 1.3378496170043945\n",
    "Epoch 2, Loss: 0.3887142539024353\n",
    "Epoch 2, Loss: 0.4099656045436859\n",
    "Epoch 3, Loss: 1.1835312843322754\n",
    "Epoch 3, Loss: 0.45539912581443787\n",
    "Epoch 3, Loss: 0.4683459401130676\n",
    "Epoch 4, Loss: 1.0939958095550537\n",
    "Epoch 4, Loss: 0.47919219732284546\n",
    "Epoch 4, Loss: 0.4713059663772583\n",
    "'This product can be better' is Negative"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
