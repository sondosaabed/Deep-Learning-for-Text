{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the ambiguity in text\n",
    "- (the it refers to who) abiguity leads to problems\n",
    "\n",
    "### Attention mechanisim\n",
    "- assigns importance to words\n",
    "- ensures that machine's interpretation align with human understanding\n",
    "- ensures human understanding\n",
    "\n",
    "### Self and multi-head attention\n",
    "- Self attention: assigns significane to words within a sentence\n",
    "    - the cat, which was on the roof, was scared\n",
    "    - the cat, was scraed\n",
    "- multi-head attention: like having multiple spotlights, capturing diffrent facets\n",
    "    - understanding \"was scared\" can relate to\n",
    "    - \"the cat\", \"the roof\" or \"was on\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n",
    "Creating a RNN model with attention\n",
    "\n",
    "At PyBooks, the team has been exploring various deep learning architectures. After some research, you decide to implement an RNN with Attention mechanism to predict the next word in a sentence. You're given a dataset with sentences and a vocabulary created from them.\n",
    "\n",
    "The following packages have been imported for you: torch, nn.\n",
    "\n",
    "The following has been preloaded for you:\n",
    "\n",
    "    vocab and vocab_size: The vocabulary set and its size\n",
    "    word_to_ix and ix_to_word: dictionary for word to index and index to word mappings\n",
    "    input_data and target_data: converted dataset to input-output pairs\n",
    "    embedding_dim and hidden_dim: dimensions for embedding and RNN hidden state\n",
    "\n",
    "You can inspect the data variable in the console to see the example sentences.\n",
    "\n",
    "    Create an embedding layer for the vocabulary with the given embedding_dim.\n",
    "    Apply a linear transformation to the RNN sequence output to get the attention scores.\n",
    "    Get the attention weights from the score.\n",
    "    Compute the context vector as the weighted sum of RNN outputs and attention weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch \n",
    "\n",
    "class RNNWithAttentionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNWithAttentionModel, self).__init__()\n",
    "        # Create an embedding layer for the vocabulary\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        # Apply a linear transformation to get the attention scores\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        #  Get the attention weights\n",
    "        attn_weights = torch.nn.functional.softmax(self.attention(out).squeeze(2), dim=1)\n",
    "        # Compute the context vector \n",
    "        context = torch.sum(attn_weights.unsqueeze(2) * out, dim=1)\n",
    "        out = self.fc(context)\n",
    "        return out\n",
    "      \n",
    "attention_model = RNNWithAttentionModel()\n",
    "optimizer = torch.optim.Adam(attention_model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"Model Instantiated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing the RNN model with attention\n",
    "\n",
    "At PyBooks, the team had previously built an RNN model for word prediction without the attention mechanism. This initial model, referred to as rnn_model, has already been trained and its instance is preloaded. Your task now is to train the new RNNWithAttentionModel and compare its predictions with that of the earlier rnn_model.\n",
    "\n",
    "The following has been preloaded for you:\n",
    "\n",
    "    inputs: list of input sequences as tensors\n",
    "    targets: tensor containing target words for each input sequence\n",
    "    optimizer: Adam optimizer function\n",
    "    criterion: CrossEntropyLoss function\n",
    "    pad_sequences: function to pad input sequences for batching\n",
    "    attention_model: defined model class from the previous exercise\n",
    "    rnn_model:trained RNN model from the team at PyBooks\n",
    "\n",
    "    Set the RNN model to evaluation mode before testing it with the test data.\n",
    "    Get the RNN output by passing the appropriate input to the RNN model.\n",
    "    Extract the word with the highest prediction score from the RNN output.\n",
    "    Similarly, for the attention model, extract the word with the highest prediction score from the attention output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    attention_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    padded_inputs = pad_sequences(inputs)\n",
    "    outputs = attention_model(padded_inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "for input_seq, target in zip(input_data, target_data):\n",
    "    input_test = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0)\n",
    "   \t\n",
    "    #  Set the RNN model to evaluation mode\n",
    "    rnn_model.eval()\n",
    "    # Get the RNN output by passing the appropriate input \n",
    "    rnn_output = rnn_model(input_test)\n",
    "    # Extract the word with the highest prediction score \n",
    "    rnn_prediction = ix_to_word[torch.argmax(rnn_output).item()]\n",
    "\n",
    "    attention_model.eval()\n",
    "    attention_output = attention_model(input_test)\n",
    "    # Extract the word with the highest prediction score\n",
    "    attention_prediction = ix_to_word[torch.argmax(attention_output).item()]\n",
    "\n",
    "    print(f\"\\nInput: {' '.join([ix_to_word[ix] for ix in input_seq])}\")\n",
    "    print(f\"Target: {ix_to_word[target]}\")\n",
    "    print(f\"RNN prediction: {rnn_prediction}\")\n",
    "    print(f\"RNN with Attention prediction: {attention_prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: the cat sat on the\n",
    "Target: mat\n",
    "RNN prediction: mat\n",
    "RNN with Attention prediction: mat\n",
    "\n",
    "Input: dogs are very loyal\n",
    "Target: animals\n",
    "RNN prediction: animals\n",
    "RNN with Attention prediction: animals\n",
    "\n",
    "Input: parrots are colorful and\n",
    "Target: noisy\n",
    "RNN prediction: noisy\n",
    "RNN with Attention prediction: noisy\n",
    "\n",
    "Input: whales are the largest\n",
    "Target: mammals\n",
    "RNN prediction: mat\n",
    "RNN with Attention prediction: mammals"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
