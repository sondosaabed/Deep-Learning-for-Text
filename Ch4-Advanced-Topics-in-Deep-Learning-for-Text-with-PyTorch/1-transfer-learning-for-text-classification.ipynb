{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning for text classification\n",
    "- pre-existing knowledge from one task to a related task.\n",
    "- Saves time, share expertise, reduces need for large data\n",
    "\n",
    "- example: pretrain -> transfer learning (fine tune) -> new model\n",
    "\n",
    "### BERT: bidirectional encoder representations from transformers\n",
    "- trained for langaue modeling\n",
    "- multiple layers of transformers\n",
    "- pre-trained on large texts\n",
    "\n",
    "if the softmax probability is less than zero then the sentimnt is negative else is positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n",
    "Transfer learning using BERT\n",
    "\n",
    "At PyBooks, the company has decided to leverage the power of the BERT model, a pre-trained transformer model, for sentiment analysis. BERT has seen remarkable performance across various NLP tasks, making it a prime candidate for this use case.\n",
    "\n",
    "You're tasked with setting up a basic workflow using the BERT model from the transformers library for binary sentiment classification.\n",
    "\n",
    "The following has been imported for you: BertTokenizer, BertForSequenceClassification, torch. The example data texts and corresponding labels are also preloaded.\n",
    "\n",
    "    Load the bert-base-uncased tokenizer and model suitable for binary classification.\n",
    "    Tokenize your dataset and prepare it for the model, ensuring it returns PyTorch tensors using the return_tensors argument.\n",
    "    Setup the optimizer using model parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import BertTokenizer, BertForSequenceClassification\n",
    "# Load the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Tokenize your data and return PyTorch tensors\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=32)\n",
    "inputs[\"labels\"] = torch.tensor(labels)\n",
    "\n",
    "# Setup the optimizer using model parameters\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00001)\n",
    "model.train()\n",
    "for epoch in range(2):\n",
    "    outputs = model(**inputs)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the BERT model\n",
    "\n",
    "Having tokenized the sample reviews using BERT's tokenizer, it's now time to evaluate the BERT model with the samples at PyBooks. Additionally, you will evaluate the model's sentiment prediction on new data.\n",
    "\n",
    "The following has been imported for you: BertTokenizer, BertForSequenceClassification, torch. The trained model instance is also preloaded. We will now test it on a new data sample.\n",
    "\n",
    "    Prepare the evaluation text for the model by tokenizing it and returning PyTorch tensors.\n",
    "    Convert the output logits to probabilities between zero and one.\n",
    "    Display the sentiments from the probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I had an awesome day!\"\n",
    "\n",
    "# Tokenize the text and return PyTorch tensors\n",
    "input_eval = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=32)\n",
    "outputs_eval = model(**input_eval)\n",
    "\n",
    "# Convert the output logits to probabilities\n",
    "predictions = torch.nn.functional.softmax(outputs_eval.logits, dim=-1)\n",
    "\n",
    "# Display the sentiments\n",
    "predicted_label = 'positive' if torch.argmax(predictions) > 0 else 'negative'\n",
    "print(f\"Text: {text}\\nSentiment: {predicted_label}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
