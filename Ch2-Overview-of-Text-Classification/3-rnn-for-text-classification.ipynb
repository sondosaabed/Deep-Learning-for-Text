{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent neural networks for text classification\n",
    "- Handle sequences of varying lengths\n",
    "- Maintain an internal short-term memory\n",
    "- CNNs spot patterns in chunks\n",
    "- RNNs rembers past words for greater meaning\n",
    "- sequential captures teh context and order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN variations: LSTM\n",
    "- Complex sentences\n",
    "- LSTMs excel at capturing such complexity\n",
    "\n",
    "### LSTMs architicure\n",
    "- Input gate\n",
    "- forget gate\n",
    "- output gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        output = self.fc(hidden.squeeze(0))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN variation: GRU \n",
    "- without needing the full context they can recognizethe spaming.\n",
    "- spam detection\n",
    "- sentiment analysis\n",
    "- text summerization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        _, hidden = self.gru(x)\n",
    "        output = self.fc(hidden.squeeze(0))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building an RNN model for text\n",
    "\n",
    "As a data analyst at PyBooks, you often encounter datasets that contain sequential information, such as customer interactions, time series data, or text documents. RNNs can effectively analyze and extract insights from such data. In this exercise, you will dive into the Newsgroup dataset that has already been processed and encoded for you. This dataset comprises articles from different categories. Your task is to apply an RNN to classify these articles into three categories:\n",
    "\n",
    "rec.autos, sci.med, and comp.graphics.\n",
    "\n",
    "The following has been loaded for you: torch, nn, optim.\n",
    "\n",
    "Additionally, the parameters input_size, hidden_size (32), num_layers (2), and num_classes have been preloaded for you.\n",
    "\n",
    "This and the following exercises use the fetch_20newsgroups dataset from sklearn.\n",
    "\n",
    "    Complete the RNN class with an RNN layer and a fully connected linear layer.\n",
    "    Initialize the model.\n",
    "    Train the RNN model for ten epochs by zeroing the gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "\n",
    "# Complete the RNN class\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = out[:, -1, :] \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "rnn_model = RNNModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model for ten epochs and zero the gradients\n",
    "for epoch in range(10): \n",
    "    optimizer.zero_grad()\n",
    "    outputs = rnn_model(X_train_seq)\n",
    "    loss = criterion(outputs, y_train_seq)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building an LSTM model for text\n",
    "\n",
    "At PyBooks, the team is constantly seeking to enhance the user experience by leveraging the latest advancements in technology. In line with this vision, they have assigned you a critical task. The team wants you to explore the potential of another powerful tool: LSTM, known for capturing more complexities in data patterns. You are working with the same Newsgroup dataset, with the objective remaining unchanged: to classify news articles into three distinct categories:\n",
    "\n",
    "rec.autos, sci.med, and comp.graphics.\n",
    "\n",
    "The following packages have been loaded for you: torch, nn, optim.\n",
    "\n",
    "    Set up an LSTM model by completing the LSTM and linear layers with the necessary parameters.\n",
    "    Initialize the model with the necessary parameters.\n",
    "    Train the LSTM model resetting the gradients to zero and passing the input data X_train_seq through the model.\n",
    "    Calculate the loss based on the predicted outputs and the true labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LSTM and the output layer with parameters\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = out[:, -1, :] \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Initialize model with required parameters\n",
    "lstm_model = LSTMModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model by passing the correct parameters and zeroing the gradient\n",
    "for epoch in range(10): \n",
    "    optimizer.zero_grad()\n",
    "    outputs = lstm_model(X_train_seq)\n",
    "    loss = criterion(outputs, y_train_seq)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a GRU model for text\n",
    "\n",
    "At PyBooks, the team has been impressed with the performance of the two models you previously trained. However, in their pursuit of excellence, they want to ensure the selection of the absolute best model for the task at hand. Therefore, they have asked you to further expand the project by experimenting with the capabilities of GRU models, renowned for their efficiency and effectiveness in text classification tasks. Your new assignment is to apply the GRU model to classify articles from the Newsgroup dataset into the following categories:\n",
    "\n",
    "rec.autos, sci.med, and comp.graphics.\n",
    "\n",
    "The following packages have been loaded for you: torch, nn, optim.\n",
    "\n",
    "    Complete the GRU class with the required parameters.\n",
    "    Initialize the model with the same parameters.\n",
    "    Train the model: pass the parameters to the criterion function, and backpropagate the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the GRU model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)       \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size) \n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = out[:, -1, :] \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "gru_model = GRUModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(gru_model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model and backpropagate the loss after initialization\n",
    "for epoch in range(15): \n",
    "    optimizer.zero_grad()\n",
    "    outputs = gru_model(X_train_seq)\n",
    "    loss = criterion(outputs, y_train_seq)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
