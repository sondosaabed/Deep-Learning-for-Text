{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "- Generate predictions and store them\n",
    "- Take the maximum\n",
    "\n",
    "**Accuracy**: the ratio of correct preduction to the total predictions\n",
    "\n",
    "**Precision**: confidence in labeling a review as negative\n",
    "\n",
    "correctly predicted positive observations/total predicted positives\n",
    "\n",
    "**Recall**: how well the model spots negative reviews\n",
    "\n",
    "correctly predicted postive obsercations/ all observations in the positive class\n",
    "\n",
    "**F1scoer**: balance between percision and recall\n",
    "\n",
    "Considerations: multiclass scores may be identical: can indicate good model performacne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6666666865348816\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "actual = torch.tensor([0, 1, 1, 0, 1, 0])\n",
    "predicted = torch.tensor([0, 0, 1, 0, 1, 1])\n",
    "accuracy = Accuracy(task=\"binary\", num_classes=2)\n",
    "acc = accuracy(predicted, actual)\n",
    "print(f\"Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6666666865348816\n",
      "Recall: 0.6666666865348816\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics import Precision, Recall\n",
    "precision = Precision(task=\"binary\", num_classes=2)\n",
    "recall = Recall(task=\"binary\", num_classes=2)\n",
    "prec = precision(predicted, actual)\n",
    "rec = recall(predicted, actual)\n",
    "print(f\"Precision: {prec}\")\n",
    "print(f\"Recall: {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating RNN classification models\n",
    "\n",
    "The team at PyBooks now wants you to evaluate the RNN model you created and ran using the Newsgroup dataset. Recall, the goal was to classify the articles into one of three categories:\n",
    "\n",
    "rec.autos, sci.med, and comp.graphics.\n",
    "\n",
    "The model was trained and you printed epoch and loss for each model.\n",
    "\n",
    "Use torchmetrics to evaluate various metrics for your model. The following has been loaded for : Accuracy, Precision, Recall, F1Score.\n",
    "\n",
    "An instance of rnn_model trained in the previous exercise in preloaded for you, too.\n",
    "\n",
    "    Create an instance of each metric for multi-class classification with num_classes equal to the number of categories.\n",
    "    Generate the predictions for the rnn_model using the test data X_test_seq.\n",
    "    Calculate the metrics using the predicted classes and the true label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the metrics\n",
    "accuracy = Accuracy(task=\"multiclass\", num_classes=3)\n",
    "precision = Precision(task=\"multiclass\", num_classes=3)\n",
    "recall = Recall(task=\"multiclass\", num_classes=3)\n",
    "f1 = F1Score(task=\"multiclass\", num_classes=3)\n",
    "\n",
    "# Generate the predictions\n",
    "outputs = rnn_model(X_test_seq)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy_score = accuracy(predicted, y_test_seq)\n",
    "precision_score = precision(predicted, y_test_seq)\n",
    "recall_score = recall(predicted, y_test_seq)\n",
    "f1_score = f1(predicted, y_test_seq)\n",
    "print(\"RNN Model - Accuracy: {}, Precision: {}, Recall: {}, F1 Score: {}\".format(accuracy_score, precision_score, recall_score, f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model's performance\n",
    "\n",
    "The PyBooks team has been making strides on the book recommendation engine. The modeling team has provided you two different models ready for your book recommendation engine at PyBooks. One model is based on LSTM (lstm_model) and the other uses a GRU (gru_model). You've been tasked to evaluate and compare these models.\n",
    "\n",
    "The testing labels y_test and the model's predictions y_pred_lstm for lstm_model and y_pred_gru for gru_model.\n",
    "\n",
    "    Define accuracy, precision, recall and F1 for multi-class classification by specifying num_classes and task.\n",
    "    Calculate and print the accuracy, precision, recall, and F1 score for lstm_model.\n",
    "    Similarly, calculate the evaluation metrics for gru_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the metrics\n",
    "accuracy = Accuracy(task=\"multiclass\", num_classes=3)\n",
    "precision = Precision(task=\"multiclass\", num_classes=3)\n",
    "recall = Recall(task=\"multiclass\", num_classes=3)\n",
    "f1 = F1Score(task=\"multiclass\", num_classes=3)\n",
    "\n",
    "# Calculate metrics for the LSTM model\n",
    "accuracy_1 = accuracy(y_pred_lstm, y_test)\n",
    "precision_1 = precision(y_pred_lstm, y_test)\n",
    "recall_1 = recall(y_pred_lstm, y_test)\n",
    "f1_1 = f1(y_pred_lstm, y_test)\n",
    "print(\"LSTM Model - Accuracy: {}, Precision: {}, Recall: {}, F1 Score: {}\".format(accuracy_1, precision_1, recall_1, f1_1))\n",
    "\n",
    "# Calculate metrics for the GRU model\n",
    "accuracy_2 = accuracy(y_pred_gru, y_test)\n",
    "precision_2 = precision(y_pred_gru, y_test)\n",
    "recall_2 = recall(y_pred_gru, y_test)\n",
    "f1_2 = f1(y_pred_gru, y_test)\n",
    "print(\"GRU Model - Accuracy: {}, Precision: {}, Recall: {}, F1 Score: {}\".format(accuracy_2, precision_2, recall_2, f1_2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
